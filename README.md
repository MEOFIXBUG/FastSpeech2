# FastSpeech 2 :airplane:
A novoice's PyTorch implementation of [**FastSpeech 2: Fast and High-Quality End-to-End Text to Speech**](https://arxiv.org/abs/2006.04558) based on FastSpeech implementation of [**Deepest-Project FastSpeech**](https://github.com/Deepest-Project/FastSpeech).
The quality of voice samples generated by this repo is not upto mark, major reason being the use of `batch_size = 8` due to inferior GPU memory and processing power. With `batch_size>8` my CUDA memory ran out.
I would be glad if anyone reading this repo can take up the training with `batch_size` as given in the paper and/or suggest ways of improving the results. :innocent: 




